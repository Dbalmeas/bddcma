# ğŸ“Š Scripts d'ingestion de donnÃ©es

## Vue d'ensemble

Ces scripts permettent d'importer les fichiers JSONL d'Everdian dans la base de donnÃ©es Supabase.

## PrÃ©requis

1. âœ… Avoir exÃ©cutÃ© le schÃ©ma SQL dans Supabase (`supabase/schema.sql`)
2. âœ… Avoir configurÃ© `.env.local` avec vos credentials Supabase
3. âœ… Avoir installÃ© les dÃ©pendances : `npm install`

## Fichiers

- **`ingest-data.ts`** : Script principal d'ingestion d'un fichier JSONL
- **`ingest-all.ts`** : Script pour ingÃ©rer tous les fichiers d'un rÃ©pertoire

## ğŸš€ Utilisation

### Option 1 : IngÃ©rer un seul fichier (RECOMMANDÃ‰ POUR TESTER)

```bash
npm run ingest -- "/Users/alexismeniante/Desktop/BDD Everdian x Albert School/2025-08-08.jsonl"
```

**Avantages** :
- Parfait pour tester sur un petit fichier
- Feedback immÃ©diat
- Facile Ã  dÃ©bugger

### Option 2 : IngÃ©rer tous les fichiers (ATTENTION : LONG)

```bash
npm run ingest:all
```

**Note** : Cela va traiter les 56 fichiers (~2.2 GB). **Temps estimÃ© : 1-2 heures**

### Option 3 : Personnaliser le rÃ©pertoire

```bash
DATA_DIR="/path/to/your/data" npm run ingest:all
```

## ğŸ“ˆ Fonctionnement

Le script fait les opÃ©rations suivantes pour chaque Ã©vÃ©nement :

1. **Parse** le JSON depuis le fichier JSONL
2. **Transforme** les donnÃ©es au format Supabase :
   - Event principal â†’ table `events`
   - Labels IA â†’ table `event_labels`
   - Localisations â†’ table `event_locations`
   - MÃ©dias (images/vidÃ©os) â†’ table `event_media`
   - Utilisateurs â†’ table `event_users`
   - MÃ©triques â†’ table `user_metrics`
3. **InsÃ¨re par batch** de 500 Ã©vÃ©nements pour optimiser les performances
4. **Affiche la progression** en temps rÃ©el

## ğŸ¯ Performance

- **Vitesse moyenne** : 100-200 Ã©vÃ©nements/seconde
- **Batch size** : 500 Ã©vÃ©nements par insertion
- **Gestion d'erreurs** : Continue mÃªme en cas d'erreur sur un Ã©vÃ©nement

### Exemple de sortie

```
ğŸš€ DÃ©marrage de l'ingestion des donnÃ©es...

ğŸ“Š Configuration:
   - Supabase URL: https://fhwflhowbhqkheeqpxqh.supabase.co
   - Batch size: 500

âœ… Connexion Supabase OK

ğŸ“‚ Traitement de 2025-08-08.jsonl...
  âœ“ 500 Ã©vÃ©nements | 150/s | 3.3s
  âœ“ 1,000 Ã©vÃ©nements | 155/s | 6.5s
  âœ“ 1,500 Ã©vÃ©nements | 152/s | 9.9s
  âœ… 2025-08-08.jsonl terminÃ© (1,842 lignes)

============================================================
ğŸ“Š STATISTIQUES FINALES
============================================================
âœ… Ã‰vÃ©nements traitÃ©s: 1,842
â­ï¸  Ã‰vÃ©nements ignorÃ©s: 0
âŒ Erreurs: 0
â±ï¸  DurÃ©e totale: 12.1s
ğŸ“ˆ Vitesse moyenne: 152 Ã©vÃ©nements/s
============================================================
```

## ğŸ§ª Test rapide

Pour tester que tout fonctionne, commencez par un petit fichier :

```bash
# 1. Tester sur le plus petit fichier (2025-09-09.jsonl = 4KB)
npm run ingest -- "/Users/alexismeniante/Desktop/BDD Everdian x Albert School/2025-09-09.jsonl"

# 2. VÃ©rifier dans la page de test
# Ouvrir http://localhost:3000/test-db
# Cliquer sur "Get Events Count"
```

Si le compteur affiche un nombre > 0, c'est que Ã§a marche ! ğŸ‰

## ğŸ”§ DÃ©pannage

### Erreur "relation does not exist"
â†’ Vous devez d'abord exÃ©cuter `supabase/schema.sql` dans l'Ã©diteur SQL de Supabase

### Erreur "Missing environment variables"
â†’ VÃ©rifiez que `.env.local` contient bien vos credentials Supabase

### Erreur "duplicate key value"
â†’ C'est normal ! Le script ignore automatiquement les doublons (upsert)

### Le script est trÃ¨s lent
â†’ Normal avec les gros fichiers (50-180 MB). Comptez 5-10 minutes par gros fichier.

### Comment arrÃªter l'ingestion ?
â†’ Appuyez sur `Ctrl+C`. Les donnÃ©es dÃ©jÃ  insÃ©rÃ©es resteront en base.

## ğŸ“Š Estimation pour l'ingestion complÃ¨te

Avec 4M+ Ã©vÃ©nements rÃ©partis sur 56 fichiers :

| Fichier | Taille | Temps estimÃ© | Ã‰vÃ©nements |
|---------|--------|--------------|------------|
| Petits (< 1 MB) | 300 KB | ~30s | ~2,000 |
| Moyens (1-50 MB) | 12-50 MB | ~5-10 min | ~50,000-250,000 |
| Gros (> 50 MB) | 50-180 MB | ~10-30 min | ~250,000-900,000 |

**DurÃ©e totale estimÃ©e** : 1-2 heures pour tout importer

## ğŸ’¡ Conseils

1. **Commencez par un petit fichier** pour tester
2. **Lancez l'ingestion complÃ¨te en arriÃ¨re-plan** (terminal dÃ©diÃ©)
3. **Surveillez les logs** pour dÃ©tecter d'Ã©ventuelles erreurs
4. **VÃ©rifiez rÃ©guliÃ¨rement** le nombre d'Ã©vÃ©nements sur `/test-db`

## ğŸ¯ Prochaines Ã©tapes

Une fois l'ingestion terminÃ©e :

1. âœ… VÃ©rifier le nombre total d'Ã©vÃ©nements dans Supabase
2. âœ… Tester les requÃªtes SQL (fonctions `search_events`, etc.)
3. âœ… DÃ©velopper l'agent IA pour interroger la base
4. âœ… CrÃ©er les visualisations automatiques

## ğŸ“ Structure des donnÃ©es

Chaque Ã©vÃ©nement JSONL est dÃ©composÃ© en :

```typescript
// 1 event â†’ multiple insertions
{
  events: 1 row,           // Table principale
  event_labels: 0-10 rows, // Labels IA (Event Temporality, Main Categories, etc.)
  event_locations: 0-5 rows, // GÃ©olocalisation (mentions + infÃ©rÃ©es)
  event_media: 0-10 rows,  // Images et vidÃ©os
  event_users: 0-1 row,    // Auteur du post
  user_metrics: 0-5 rows   // MÃ©triques (followers, rank, etc.)
}
```

## âš™ï¸ Configuration avancÃ©e

### Changer la taille des batchs

Ã‰ditez `scripts/ingest-data.ts` :

```typescript
const BATCH_SIZE = 500 // Augmentez pour plus de vitesse, diminuez si erreurs
```

### ParallÃ©liser l'ingestion

Pour aller plus vite, vous pouvez lancer plusieurs scripts en parallÃ¨le :

```bash
# Terminal 1
npm run ingest -- "path/to/file1.jsonl"

# Terminal 2
npm run ingest -- "path/to/file2.jsonl"

# etc.
```

**Attention** : Ne pas dÃ©passer 3-4 processus en parallÃ¨le pour Ã©viter de surcharger Supabase.
