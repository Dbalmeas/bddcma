/**
 * Script d'ingestion des donn√©es JSONL vers Supabase
 * Projet Everdian x Albert School
 *
 * Usage:
 *   npm run ingest -- <chemin-vers-fichier.jsonl>
 *   npm run ingest:all  (pour tous les fichiers)
 */

// Charger les variables d'environnement depuis .env.local
import { config } from 'dotenv'
import * as path from 'path'
config({ path: path.join(process.cwd(), '.env.local') })

import { createClient } from '@supabase/supabase-js'
import * as fs from 'fs'
import * as readline from 'readline'

// Configuration
const BATCH_SIZE = 500 // Nombre d'√©v√©nements par batch
const SUPABASE_URL = process.env.NEXT_PUBLIC_SUPABASE_URL!
const SUPABASE_ANON_KEY = process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!

if (!SUPABASE_URL || !SUPABASE_ANON_KEY) {
  console.error('‚ùå Variables d\'environnement manquantes!')
  console.error('Assurez-vous que NEXT_PUBLIC_SUPABASE_URL et NEXT_PUBLIC_SUPABASE_ANON_KEY sont d√©finis')
  process.exit(1)
}

const supabase = createClient(SUPABASE_URL, SUPABASE_ANON_KEY)

// Types
interface RawEvent {
  id: string
  text: string
  english_sentence?: string
  lang?: string
  labels_v2?: Array<{
    type: string
    value: string
    score: number
  }>
  publish_date?: string
  locations?: {
    mentions?: Array<any>
    inferred?: Array<any>
    post?: Array<any>
  }
  images?: string[]
  videos?: string[]
  url?: string
  user?: {
    userName?: string
    metrics?: Array<{
      metricName: string
      metricCount: number
    }>
  }
  network?: string
}

interface TransformedData {
  events: any[]
  labels: any[]
  locations: any[]
  media: any[]
  users: any[]
  metrics: any[]
}

// Statistiques
let stats = {
  totalLines: 0,
  processedEvents: 0,
  skippedEvents: 0,
  errors: 0,
  startTime: Date.now(),
  currentFile: '',
}

/**
 * Transforme un √©v√©nement brut JSONL en format Supabase
 */
function transformEvent(raw: RawEvent): TransformedData {
  const result: TransformedData = {
    events: [],
    labels: [],
    locations: [],
    media: [],
    users: [],
    metrics: [],
  }

  // Event principal
  result.events.push({
    id: raw.id,
    text: raw.text || '',
    english_sentence: raw.english_sentence || null,
    lang: raw.lang || null,
    publish_date: raw.publish_date || null,
    network: raw.network || null,
    url: raw.url || null,
  })

  // Labels
  if (raw.labels_v2 && Array.isArray(raw.labels_v2)) {
    raw.labels_v2.forEach((label) => {
      result.labels.push({
        event_id: raw.id,
        type: label.type,
        value: label.value,
        score: label.score,
      })
    })
  }

  // Locations (mentions)
  if (raw.locations?.mentions && Array.isArray(raw.locations.mentions)) {
    raw.locations.mentions.forEach((loc) => {
      result.locations.push({
        event_id: raw.id,
        location_type: 'mention',
        name: loc.name || null,
        label: loc.label || null,
        layer: loc.layer || null,
        country: loc.country || null,
        coordinates: loc.coordinates
          ? `POINT(${loc.coordinates[0]} ${loc.coordinates[1]})`
          : null,
      })
    })
  }

  // Locations (inferred)
  if (raw.locations?.inferred && Array.isArray(raw.locations.inferred)) {
    raw.locations.inferred.forEach((loc) => {
      result.locations.push({
        event_id: raw.id,
        location_type: 'inferred',
        name: loc.name || null,
        label: loc.label || null,
        layer: loc.layer || null,
        country: loc.country || null,
        coordinates: loc.coordinates
          ? `POINT(${loc.coordinates[0]} ${loc.coordinates[1]})`
          : null,
      })
    })
  }

  // Locations (post)
  if (raw.locations?.post && Array.isArray(raw.locations.post)) {
    raw.locations.post.forEach((loc) => {
      result.locations.push({
        event_id: raw.id,
        location_type: 'post',
        name: loc.name || null,
        label: loc.label || null,
        layer: loc.layer || null,
        country: loc.country || null,
        coordinates: loc.coordinates
          ? `POINT(${loc.coordinates[0]} ${loc.coordinates[1]})`
          : null,
      })
    })
  }

  // Media (images)
  if (raw.images && Array.isArray(raw.images)) {
    raw.images.forEach((url) => {
      result.media.push({
        event_id: raw.id,
        media_type: 'image',
        url,
      })
    })
  }

  // Media (videos)
  if (raw.videos && Array.isArray(raw.videos)) {
    raw.videos.forEach((url) => {
      result.media.push({
        event_id: raw.id,
        media_type: 'video',
        url,
      })
    })
  }

  // User
  if (raw.user?.userName) {
    result.users.push({
      event_id: raw.id,
      username: raw.user.userName,
    })

    // User metrics (on les ajoutera apr√®s avoir l'ID de l'utilisateur)
    if (raw.user.metrics && Array.isArray(raw.user.metrics)) {
      raw.user.metrics.forEach((metric) => {
        result.metrics.push({
          event_id: raw.id, // On stocke temporairement l'event_id
          username: raw.user!.userName,
          metric_name: metric.metricName,
          metric_count: metric.metricCount,
        })
      })
    }
  }

  return result
}

/**
 * Ins√®re un batch de donn√©es dans Supabase
 */
async function insertBatch(batch: TransformedData) {
  try {
    // 1. Ins√©rer les events
    if (batch.events.length > 0) {
      const { error: eventsError } = await supabase
        .from('events')
        .upsert(batch.events, { onConflict: 'id', ignoreDuplicates: true })

      if (eventsError) {
        console.error('Erreur insertion events:', eventsError.message)
        throw eventsError
      }
    }

    // 2. Ins√©rer les labels
    if (batch.labels.length > 0) {
      const { error: labelsError } = await supabase
        .from('event_labels')
        .insert(batch.labels, { ignoreDuplicates: true })

      if (labelsError && !labelsError.message.includes('duplicate')) {
        console.error('Erreur insertion labels:', labelsError.message)
      }
    }

    // 3. Ins√©rer les locations
    if (batch.locations.length > 0) {
      // Filtrer les locations avec coordonn√©es valides
      const validLocations = batch.locations.filter((loc) => {
        if (!loc.coordinates) return true // On garde celles sans coordonn√©es
        // V√©rifier que c'est un format valide
        return loc.coordinates.match(/^POINT\(-?\d+\.?\d* -?\d+\.?\d*\)$/)
      })

      const { error: locationsError } = await supabase
        .from('event_locations')
        .insert(validLocations, { ignoreDuplicates: true })

      if (locationsError && !locationsError.message.includes('duplicate')) {
        console.error('Erreur insertion locations:', locationsError.message)
      }
    }

    // 4. Ins√©rer les media
    if (batch.media.length > 0) {
      const { error: mediaError } = await supabase
        .from('event_media')
        .insert(batch.media, { ignoreDuplicates: true })

      if (mediaError && !mediaError.message.includes('duplicate')) {
        console.error('Erreur insertion media:', mediaError.message)
      }
    }

    // 5. Ins√©rer les users
    if (batch.users.length > 0) {
      const { error: usersError } = await supabase
        .from('event_users')
        .insert(batch.users, { ignoreDuplicates: true })

      if (usersError && !usersError.message.includes('duplicate')) {
        console.error('Erreur insertion users:', usersError.message)
      }

      // 6. R√©cup√©rer les IDs des users et ins√©rer les metrics
      if (batch.metrics.length > 0) {
        // Grouper par username pour r√©cup√©rer les IDs
        const usernames = [...new Set(batch.metrics.map((m) => m.username))]
        const { data: users } = await supabase
          .from('event_users')
          .select('id, username, event_id')
          .in('username', usernames)

        if (users && users.length > 0) {
          // Mapper les metrics avec les bons user_id
          const metricsWithIds = batch.metrics
            .map((metric) => {
              const user = users.find(
                (u) => u.username === metric.username && u.event_id === metric.event_id
              )
              if (!user) return null
              return {
                user_id: user.id,
                metric_name: metric.metric_name,
                metric_count: metric.metric_count,
              }
            })
            .filter(Boolean)

          if (metricsWithIds.length > 0) {
            await supabase
              .from('user_metrics')
              .insert(metricsWithIds, { ignoreDuplicates: true })
          }
        }
      }
    }

    stats.processedEvents += batch.events.length
  } catch (error) {
    console.error('‚ùå Erreur lors de l\'insertion du batch:', error)
    stats.errors++
    throw error
  }
}

/**
 * Traite un fichier JSONL
 */
async function processFile(filePath: string): Promise<void> {
  stats.currentFile = path.basename(filePath)
  console.log(`\nüìÇ Traitement de ${stats.currentFile}...`)

  const fileStream = fs.createReadStream(filePath, { encoding: 'utf8' })
  const rl = readline.createInterface({
    input: fileStream,
    crlfDelay: Infinity,
  })

  let batch: TransformedData = {
    events: [],
    labels: [],
    locations: [],
    media: [],
    users: [],
    metrics: [],
  }

  let lineCount = 0

  for await (const line of rl) {
    if (!line.trim()) continue

    lineCount++
    stats.totalLines++

    try {
      const raw: RawEvent = JSON.parse(line)

      // Transformer l'√©v√©nement
      const transformed = transformEvent(raw)

      // Ajouter au batch
      batch.events.push(...transformed.events)
      batch.labels.push(...transformed.labels)
      batch.locations.push(...transformed.locations)
      batch.media.push(...transformed.media)
      batch.users.push(...transformed.users)
      batch.metrics.push(...transformed.metrics)

      // Ins√©rer le batch si on a atteint la taille limite
      if (batch.events.length >= BATCH_SIZE) {
        await insertBatch(batch)

        // Afficher progression
        const elapsed = ((Date.now() - stats.startTime) / 1000).toFixed(1)
        const rate = (stats.processedEvents / (Date.now() - stats.startTime) * 1000).toFixed(0)
        console.log(
          `  ‚úì ${stats.processedEvents.toLocaleString()} √©v√©nements | ` +
          `${rate}/s | ${elapsed}s`
        )

        // R√©initialiser le batch
        batch = {
          events: [],
          labels: [],
          locations: [],
          media: [],
          users: [],
          metrics: [],
        }
      }
    } catch (error) {
      console.error(`Erreur ligne ${lineCount}:`, error)
      stats.skippedEvents++
    }
  }

  // Ins√©rer le dernier batch s'il reste des donn√©es
  if (batch.events.length > 0) {
    await insertBatch(batch)
  }

  console.log(`  ‚úÖ ${stats.currentFile} termin√© (${lineCount.toLocaleString()} lignes)`)
}

/**
 * Point d'entr√©e principal
 */
async function main() {
  console.log('üöÄ D√©marrage de l\'ingestion des donn√©es...\n')
  console.log(`üìä Configuration:`)
  console.log(`   - Supabase URL: ${SUPABASE_URL}`)
  console.log(`   - Batch size: ${BATCH_SIZE}`)
  console.log('')

  const args = process.argv.slice(2)

  if (args.length === 0) {
    console.error('‚ùå Usage: npm run ingest -- <fichier.jsonl>')
    console.error('   ou: npm run ingest:all')
    process.exit(1)
  }

  const filePath = args[0]

  if (!fs.existsSync(filePath)) {
    console.error(`‚ùå Fichier introuvable: ${filePath}`)
    process.exit(1)
  }

  try {
    // Tester la connexion Supabase
    const { error } = await supabase.from('events').select('count').limit(1)
    if (error) {
      console.error('‚ùå Erreur de connexion Supabase:', error.message)
      console.error('Assurez-vous que le sch√©ma SQL a √©t√© ex√©cut√©.')
      process.exit(1)
    }

    console.log('‚úÖ Connexion Supabase OK\n')

    // Traiter le fichier
    await processFile(filePath)

    // Afficher statistiques finales
    const duration = ((Date.now() - stats.startTime) / 1000).toFixed(1)
    const avgRate = (stats.processedEvents / (Date.now() - stats.startTime) * 1000).toFixed(0)

    console.log('\n' + '='.repeat(60))
    console.log('üìä STATISTIQUES FINALES')
    console.log('='.repeat(60))
    console.log(`‚úÖ √âv√©nements trait√©s: ${stats.processedEvents.toLocaleString()}`)
    console.log(`‚è≠Ô∏è  √âv√©nements ignor√©s: ${stats.skippedEvents.toLocaleString()}`)
    console.log(`‚ùå Erreurs: ${stats.errors}`)
    console.log(`‚è±Ô∏è  Dur√©e totale: ${duration}s`)
    console.log(`üìà Vitesse moyenne: ${avgRate} √©v√©nements/s`)
    console.log('='.repeat(60))

  } catch (error) {
    console.error('\n‚ùå ERREUR FATALE:', error)
    process.exit(1)
  }
}

// Ex√©cution
main().catch(console.error)
